{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895bf4d8",
   "metadata": {},
   "source": [
    "## Importing the Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df7c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image preprocessing import image data generator\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d84686",
   "metadata": {},
   "source": [
    "## Part -1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8e83d",
   "metadata": {},
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484465e6",
   "metadata": {},
   "source": [
    " * to preprocesses image we need to apply transformations on images of trainingset\n",
    "* we wont apply on test set\n",
    "* transformation to avoid overfitting\n",
    "* if no transformation applied we will get accuracy difference between training and test sets will cause overfitting\n",
    "* Geometric trans like transvection to shift some pixels, rotate images, horizontal flips, zoomin zoom out\n",
    "* Called as Image Augmentation\n",
    "* by applying these we will get new images and model wont be overtrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de81719",
   "metadata": {},
   "source": [
    "* ImageDataGenerator will generate batches of tensor image data with real time data augmentation\n",
    "* rescale = 1/255 it is about feature scaling..it will apply feature scaling to each and every one of your pixels by dividing their value by 255 because each pixels takes a value between 0-255 ...we are normalizing it\n",
    "* feature scaling is most important in neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ccb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "    rescale=1./255,shear_range=0.2,zoom_range=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aca2b0",
   "metadata": {},
   "source": [
    "* flow from directory is a method\n",
    "* binary outcome so class mode is binary else categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df6bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(directory='dataset/training_set',\n",
    "                                                   target_size = (64,64), batch_size = 32,\n",
    "                                                   class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af47581",
   "metadata": {},
   "source": [
    "* We should not apply same transformations on test like shear,zoom etc because there\n",
    "like new images and to avoid info leakage\n",
    "* but we have to rescale their pixels\n",
    "* like for previous we applied fit_transform on training and transform on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eda5df",
   "metadata": {},
   "source": [
    "### Preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316b0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(directory='dataset/test_set',\n",
    "                                           target_size=(64,64), batch_size = 32,\n",
    "                                           class_mode = 'binary') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13a729",
   "metadata": {},
   "source": [
    "## Part - 2 Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92edb366",
   "metadata": {},
   "source": [
    "### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8388313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01584a9",
   "metadata": {},
   "source": [
    "### Step - 1 Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab55d9",
   "metadata": {},
   "source": [
    "* The convolution layer we are adding will be an object of a certain class called conv 2D class\n",
    "* in conv2d we have to input number of filters(feature detectors), kernel_size is matrix size = 3 = 3x3\n",
    "* input shape of your inputs and since we resized in part 1 it will be 64x64x3 \n",
    "3 as we are coloured... and b&w we will have 64x64x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95683d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32,\n",
    "                              kernel_size = 3,\n",
    "                              activation = 'relu',\n",
    "                              input_shape = [64,64,3]))\n",
    "\n",
    "# classic  = 32 filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6677bc",
   "metadata": {},
   "source": [
    "### Step - 2 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea0139",
   "metadata": {},
   "source": [
    "* the arguments are pool size = 2x2 matrix, strides = 2 shifted by 2\n",
    "* we are applying max pooling\n",
    "* padding default value -- now values are assigned 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38325227",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dd44a",
   "metadata": {},
   "source": [
    "### Adding a second convolution layer and pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f831249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the input shape layer as it will come only when first layer is input\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,\n",
    "                              kernel_size = 3,\n",
    "                              activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6535bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2,\n",
    "                                 strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2cd3b",
   "metadata": {},
   "source": [
    "### Step - 3 Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad22682",
   "metadata": {},
   "source": [
    "* Flattening the result of all these convolutions and pooling into a one dimensional vector\n",
    "which will become input of future connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c9b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten()) # no arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce14f84",
   "metadata": {},
   "source": [
    "### Step - 4 Full Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d55e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the skeleton\n",
    "cnn.add(tf.keras.layers.Dense(units = 128 ,\n",
    "                              activation = 'relu'))\n",
    "# units is number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0e206",
   "metadata": {},
   "source": [
    "### Step - 5 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6049d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1 ,\n",
    "                              activation = 'sigmoid'))\n",
    "## binary clasf - sigmoid\n",
    "## multiclass clasf - softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf811aa4",
   "metadata": {},
   "source": [
    "## Part 3 - Training The CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a2ec265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e4284",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f209b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam',\n",
    "            loss = 'binary_crossentropy', \n",
    "            metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fce67c",
   "metadata": {},
   "source": [
    "### Training the CNN on trainingset and evalutaing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c80ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 55s 214ms/step - loss: 0.6624 - accuracy: 0.6066 - val_loss: 0.5932 - val_accuracy: 0.7045\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 46s 185ms/step - loss: 0.6026 - accuracy: 0.6798 - val_loss: 0.5739 - val_accuracy: 0.7145\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 0.5514 - accuracy: 0.7159 - val_loss: 0.6262 - val_accuracy: 0.6520\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.5240 - accuracy: 0.7391 - val_loss: 0.4959 - val_accuracy: 0.7680\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.4952 - accuracy: 0.7602 - val_loss: 0.4931 - val_accuracy: 0.7650\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.4791 - accuracy: 0.7645 - val_loss: 0.4959 - val_accuracy: 0.7725\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4593 - accuracy: 0.7837 - val_loss: 0.4587 - val_accuracy: 0.7940\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4552 - accuracy: 0.7826 - val_loss: 0.4528 - val_accuracy: 0.7965\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4381 - accuracy: 0.7921 - val_loss: 0.5309 - val_accuracy: 0.7610\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 0.4269 - accuracy: 0.7980 - val_loss: 0.5491 - val_accuracy: 0.7325\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4149 - accuracy: 0.8062 - val_loss: 0.4816 - val_accuracy: 0.7810\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4097 - accuracy: 0.8084 - val_loss: 0.5169 - val_accuracy: 0.7650\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4025 - accuracy: 0.8142 - val_loss: 0.4367 - val_accuracy: 0.8035\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.3825 - accuracy: 0.8241 - val_loss: 0.4322 - val_accuracy: 0.8075\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.3848 - accuracy: 0.8267 - val_loss: 0.4748 - val_accuracy: 0.7850\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.3714 - accuracy: 0.8366 - val_loss: 0.4385 - val_accuracy: 0.8060\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.3685 - accuracy: 0.8340 - val_loss: 0.4384 - val_accuracy: 0.8055\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 0.3512 - accuracy: 0.8421 - val_loss: 0.4440 - val_accuracy: 0.8080\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.3450 - accuracy: 0.8503 - val_loss: 0.4813 - val_accuracy: 0.7790\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.3436 - accuracy: 0.8443 - val_loss: 0.4446 - val_accuracy: 0.8080\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.3279 - accuracy: 0.8572 - val_loss: 0.4375 - val_accuracy: 0.8035\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.3221 - accuracy: 0.8539 - val_loss: 0.4772 - val_accuracy: 0.7925\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.3128 - accuracy: 0.8683 - val_loss: 0.4767 - val_accuracy: 0.8085\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.3064 - accuracy: 0.8671 - val_loss: 0.4771 - val_accuracy: 0.8015\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.2924 - accuracy: 0.8749 - val_loss: 0.4530 - val_accuracy: 0.8070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12125b33970>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294cc7",
   "metadata": {},
   "source": [
    "## Part - 4 Making a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34bfaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from keras.preprocessing import image\n",
    "from tensorflow.keras.utils import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0af5ff",
   "metadata": {},
   "source": [
    "* we will have to load the test image in pil format\n",
    "* has to have same size as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c58f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = load_img('dataset/single_prediction/cat_or_dog_4.jpg', \n",
    "                      target_size=(64,64) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68276ba5",
   "metadata": {},
   "source": [
    "* In order to be accepted by predict method we have to convert PIL format to an array\n",
    "* 2D array\n",
    "* image to array from image preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c1031ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "656e76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_array = img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5fe87638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 45.,  30.,  27.],\n",
       "        [ 38.,  24.,  21.],\n",
       "        [ 25.,  14.,  10.],\n",
       "        ...,\n",
       "        [ 35.,  17.,   5.],\n",
       "        [ 32.,   7.,   0.],\n",
       "        [ 34.,   9.,   2.]],\n",
       "\n",
       "       [[ 47.,  29.,  27.],\n",
       "        [ 34.,  25.,  20.],\n",
       "        [ 24.,  14.,   5.],\n",
       "        ...,\n",
       "        [ 37.,  17.,   8.],\n",
       "        [ 32.,   7.,   2.],\n",
       "        [ 34.,   9.,   2.]],\n",
       "\n",
       "       [[ 45.,  27.,  25.],\n",
       "        [ 31.,  22.,  17.],\n",
       "        [ 24.,  13.,   9.],\n",
       "        ...,\n",
       "        [ 38.,  18.,   9.],\n",
       "        [ 32.,   7.,   2.],\n",
       "        [ 34.,   9.,   2.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[114., 101.,  92.],\n",
       "        [ 80.,  68.,  56.],\n",
       "        [ 51.,  34.,  27.],\n",
       "        ...,\n",
       "        [ 49.,  28.,   0.],\n",
       "        [ 32.,   7.,   2.],\n",
       "        [ 32.,   7.,   0.]],\n",
       "\n",
       "       [[145., 132., 123.],\n",
       "        [107.,  93.,  80.],\n",
       "        [ 68.,  53.,  32.],\n",
       "        ...,\n",
       "        [ 48.,  29.,   0.],\n",
       "        [ 32.,   7.,   2.],\n",
       "        [ 32.,   7.,   0.]],\n",
       "\n",
       "       [[163., 156., 146.],\n",
       "        [128., 116., 102.],\n",
       "        [ 89.,  70.,  56.],\n",
       "        ...,\n",
       "        [ 49.,  30.,   0.],\n",
       "        [ 32.,   8.,   0.],\n",
       "        [ 32.,   7.,   2.]]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf265113",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47c742f",
   "metadata": {},
   "source": [
    "* The predict method has to be called on exact same format that was used in training\n",
    "* Our CNN was not trained on single images it was trained on batches\n",
    "* even if it is single it should be in batch\n",
    "* So we have extra dimension of the batch_size = 32\n",
    "* So we are adding extra dimension. axis = 0 means 1st dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6dea64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_array = np.expand_dims(test_image_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3af85947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "result = cnn.predict(test_image_array/255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "138098a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out put will be 0 or 1\n",
    "# class indices will help to get know what 1 and 0 is\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "81da9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result also has batch dimension so first get acces to batch[0] and inside batch\n",
    "# get access to first and only element of batch [0]\n",
    "if result[0][0] > 0.5:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'Cat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "642efd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dog'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5ee86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
